import streamlit as st
import os
from core.auto_crawler_iter.iteration_engine import CrawlerIterationEngine
from core.auto_crawler_iter.metrics_collector import MetricsCollector
from scrapers.logger import log_info

st.header("ğŸ§¬ çˆ¬è™«è‡ªæˆ‘è¿­ä»£æ§åˆ¶å°")

engine = CrawlerIterationEngine()
collector = MetricsCollector()

col1, col2 = st.columns(2)
with col1:
    if st.button("è¿è¡Œä¸€è½®è¿­ä»£"):
        result = engine.run_once()
        st.write(result)
with col2:
    metrics = collector.collect()
    st.subheader("å½“å‰æŒ‡æ ‡")
    st.json(metrics)

st.divider()
st.subheader("å€™é€‰è¡¥ä¸åˆ—è¡¨")
patch_dir = engine.cfg["patch_output_dir"]
patches = []
if os.path.isdir(patch_dir):
    patches = [f for f in os.listdir(patch_dir) if f.endswith(".patch")]
if not patches:
    st.info("æš‚æ— è¡¥ä¸å€™é€‰ã€‚ç‚¹å‡»ä¸Šæ–¹â€œè¿è¡Œä¸€è½®è¿­ä»£â€ç”Ÿæˆã€‚")
else:
    for p in patches:
        tag = p.replace(".patch", "")
        with st.expander(f"è¡¥ä¸: {p}"):
            st.code(open(os.path.join(patch_dir, p), "r", encoding="utf-8").read(), language="diff")
            apply = st.button(f"åº”ç”¨è¡¥ä¸ {tag}")
            if apply:
                res = engine.apply_patch(tag)
                st.success(res)

st.divider()
st.caption("æç¤ºï¼šè¡¥ä¸åªä¿®æ”¹ scrapers/amazon_scraper.pyï¼Œç”Ÿæˆæ—¶å†™å…¥ sandbox ç›®å½•ä»¥åŠ diff è¡¥ä¸ï¼Œéœ€æ‰‹åŠ¨åº”ç”¨ã€‚")